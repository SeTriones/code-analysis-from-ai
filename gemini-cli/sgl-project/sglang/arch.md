# SGLang MoE Runner Architecture

This document details the Mixture-of-Experts (MoE) runner modes supported in SGLang and their application scenarios.

## Supported MoE Runner Modes

SGLang supports **9 modes** (backends) for the MoE runner, defined in `MoeRunnerBackend`:

1.  **`auto`** (Default)
2.  **`deep_gemm`**
3.  **`triton`**
4.  **`triton_kernel`**
5.  **`flashinfer_trtllm`**
6.  **`flashinfer_cutlass`**
7.  **`flashinfer_mxfp4`**
8.  **`flashinfer_cutedsl`**
9.  **`cutlass`**

## Application Scenarios

These modes can be selected via the `--moe-runner-backend` server argument.

| Mode | Scenario & Application | Parallelism (MoE FFN) | Supported Quantization |
| :--- | :--- | :--- | :--- |
| **`auto`** | **General Usage.** This is the default setting. SGLang automatically selects the most appropriate backend based on your hardware (e.g., NVIDIA vs. AMD), installed libraries (`flashinfer`, `deep_gemm`), and model configuration (quantization type). | Auto-selected | Auto-selected |
| **`deep_gemm`** | **DeepSeek Models (Hopper+).** Specifically optimized for DeepSeek-V3 and DeepSeek-R1 models, typically delivering the highest performance on NVIDIA Hopper (H100/H200) and Blackwell GPUs. Requires the `deep_gemm` library. | Expert Parallel (EP) via DeepEP | FP8 (W8A8) |
| **`triton`** | **Standard / Portable.** A general-purpose backend using Triton kernels. It serves as a robust baseline and is often used when specific high-performance kernels (like FlashInfer) are unavailable or incompatible with the current setup. | Tensor Parallel (TP) | FP16/BF16, FP8 (W8A8), INT8 (W8A8), INT8 (W8A16), INT4 (W4A16) |
| **`triton_kernel`** | **Optimized Triton.** A variation of the Triton backend using specific optimized kernels (likely from `sgl-kernel` or internal implementations) for better performance than the standard Triton path in some cases. | Tensor Parallel (TP) | FP16/BF16, FP8, INT8, INT4 |
| **`flashinfer_trtllm`** | **High-Performance FP8/FP4.** Leverages FlashInfer with TensorRT-LLM kernels. This is critical for running quantized models like **DeepSeek-V3 in FP4**, providing significant speedups on supported hardware. | Tensor Parallel (TP) | FP8 (W8A8), FP4 (NVFP4) |
| **`flashinfer_cutlass`** | **FlashInfer + Cutlass.** Used when FlashInfer is available but Cutlass kernels are preferred for specific matrix multiplications, often for specific quantization formats or attention mechanisms coupled with MoE. | Expert Parallel (EP) + Tensor Parallel (TP) | FP4 (NVFP4) |
| **`flashinfer_mxfp4`** | **MXFP4 Quantization.** Specifically designed for models quantized using the MXFP4 (Microscaling formats) standard. | Expert Parallel (EP) + Tensor Parallel (TP) | MXFP4 |
| **`flashinfer_cutedsl`** | **Experimental / CuTe.** A backend using FlashInfer with CuTeDSL (CUDA Template Library), offering high-performance primitives for specific GPU architectures. | Expert Parallel (EP) + Tensor Parallel (TP) | FP4 (NVFP4) |
| **`cutlass`** | **FP8 / GEMM.** A generic backend using Cutlass kernels. It is often used for **FP8 MoE** implementations (`Fp8MoEMethod`) when FlashInfer is not used or for specific GEMM (General Matrix Multiply) operations. | Tensor Parallel (TP) | FP8 (W8A8), FP4 (NVFP4) |

## Configuration

You can explicitly set the backend when launching the server:

```bash
python -m sglang.launch_server --model-path <path> --moe-runner-backend deep_gemm
```

(Replace `deep_gemm` with your desired mode).

# Speculative Decoding Architecture

Speculative decoding in SGLang is implemented primarily through the **EAGLE** (Extrapolation Algorithm for Greater Language-model Efficiency) framework, alongside other variants like EAGLE3, NextN, and N-gram.

The implementation is split between the **Python runtime** (`python/sglang/srt`) which orchestrates the draft-verify loop, and **CUDA kernels** (`sgl-kernel`) which handle high-performance tree construction and sampling.

## Core Logic & Workflow

The process follows a "Draft-Verify" cycle managed by the `EAGLEWorker`.

### 1. Draft Phase (`EAGLEWorker.draft`)
*   The draft model (a smaller model) runs for `speculative_num_steps`.
*   At each step, it expands a tree of candidate tokens using `select_top_k_tokens`.
*   The tree structure is flattened into efficient tensors using the `build_tree_kernel_efficient` kernel.

### 2. Verify Phase (`EAGLEWorker.verify`)
The target model (the main LLM) validates the speculative tokens generated by the draft model.

*   **Preparation**:
    *   **Input Replacement**: `batch.input_ids` is replaced with `self.draft_token` (all candidate tokens generated by the draft phase).
    *   **Memory Allocation**: New KV cache slots are optimistically allocated for *all* candidate tokens using `alloc_token_slots`.
    *   **Mapping**: The request-to-token pool mapping is updated so the attention backend knows where to write the K/V pairs.

*   **Target Execution**:
    *   The target model runs a forward pass (`forward_batch_generation`) with `is_verify=True`, computing logits for all candidate tokens in parallel.

*   **Verification Logic**:
    *   **Kernel Execution**: The `tree_speculative_sampling_target_only` CUDA kernel traverses the token tree.
    *   **Comparison**: It compares target probabilities (`target_probs`) against draft probabilities (implicit in the tree structure) using rejection sampling.
    *   **Output**: Returns indices of accepted tokens (`accept_index`), verified token IDs (`predict`), and acceptance lengths (`accept_length`).

*   **State Update & Cleanup**:
    *   Accepted tokens are appended to the request's output.
    *   **Eviction**: A boolean mask marks rejected tokens. If `Page Size=1` or `TopK=1`, slots are freed directly. For complex cases (`Page Size>1` & `TopK>1`), accepted tokens are moved to contiguous blocks using `move_kv_cache` before freeing the fragmentation.

## Key Code Files

### Python Runtime (`python/sglang/srt/speculative/`)
*   **`eagle_worker.py`**: The central controller. Inherits from `TpModelWorker` and implements the `draft()` and `verify()` loops.
*   **`eagle_info.py`**: Defines core data structures (`EagleDraftInput`, `EagleVerifyInput`) and contains the `verify()` method calling the sampling kernels.
*   **`spec_info.py`**: Registry for speculative algorithms (`SpeculativeAlgorithm`).
*   **`spec_utils.py`**: Utility functions for tree expansion (`select_top_k_tokens`), memory assignment (`assign_req_to_token_pool`), and cache location management (`get_src_tgt_cache_loc`).

### Kernel Interface (`sgl-kernel/`)
*   **`python/sgl_kernel/speculative.py`**: Python bindings.
*   **`csrc/speculative/speculative_sampling.cu`**: CUDA implementation of rejection sampling (`TreeSpeculativeSamplingTargetOnly`) and tree construction.

## Core Data Structures

### `EagleVerifyInput`
Prepares the batch for the target model's verification pass.
*   `draft_token`: Candidate tokens proposed by the draft model.
*   `custom_mask`: Attention mask representing the tree structure.
*   `retrive_index`: Indices mapping tree nodes back to requests.
*   `retrive_next_token`: Tree topology (child node index).
*   `retrive_next_sibling`: Tree topology (sibling node index).

### `EagleDraftInput`
Maintains state during the autoregressive drafting steps.
*   `hidden_states`: Hidden states from the model.
*   `verified_id`: Tokens confirmed in the previous step.
*   `topk_p` / `topk_index`: Probabilities and indices of top-k candidates.

### Verification Process Detail

The verification phase is where the target model (the main LLM) validates the speculative tokens generated by the draft model. This process involves parallel execution, kernel-based tree verification, and complex memory management.

#### 1. High-Level Flow
The process follows these main stages:
1.  **Preparation**: Setting up the batch to treat draft tokens as inputs and allocating memory.
2.  **Target Execution**: Running the target model to compute logits for all candidate tokens in parallel.
3.  **Verification Logic**: Comparing target logits with draft probabilities to determine accepted tokens.
4.  **State Update**: Appending accepted tokens to the request and handling finish conditions.
5.  **Memory Cleanup**: Freeing the memory of rejected tokens and compacting the KV cache.

#### 2. Detailed Execution Steps

**Step 1: Preparation (`EagleVerifyInput.prepare_for_verify` in `eagle_info.py`)**
*   **Input Replacement**: The standard `batch.input_ids` are replaced with `self.draft_token`, which contains all candidate tokens generated during the draft phase.
*   **Optimistic Memory Allocation**: New KV cache slots are allocated for *all* candidate tokens using `alloc_token_slots` (or `alloc_paged_token_slots_extend` for paged attention). This assumes all tokens *might* be accepted.
*   **Mapping Update**: The request-to-token pool mapping is updated so that the attention backend knows where to write the Key/Value pairs for these new tokens.

**Step 2: Target Model Execution (`EAGLEWorker.verify` in `eagle_worker.py`)**
*   The target worker executes `forward_batch_generation` with the flag `is_verify=True`.
*   The target model computes logits for all provided `draft_tokens` in a single parallel forward pass.
*   The result is `logits_output`, containing `next_token_logits` for every candidate token.

**Step 3: Verification Logic (`EagleVerifyInput.verify` in `eagle_info.py`)**
*   **Logit Processing**: Penalties and grammar masks are applied to `logits_output` if necessary.
*   **Probability Calculation**: Target probabilities (`target_probs`) are computed via softmax.
*   **Kernel Execution**: The custom CUDA kernel `tree_speculative_sampling_target_only` (in `sgl-kernel/csrc/speculative/speculative_sampling.cu`) is invoked.
    *   This kernel traverses the token tree structure.
    *   It compares `target_probs` against the draft probabilities (implicit in the tree topology) using rejection sampling (or greedy matching if temperature is 0).
    *   **Output**: It produces `accept_index` (indices of accepted tokens), `predict` (verified token IDs), and `accept_length` (number of accepted tokens per request).

**Step 4: Request State Update**
*   The system iterates through the accepted tokens.
*   Accepted token IDs are appended to `req.output_ids`.
*   **Finish Check**: If a request generates an EOS token or hits the max length limit *within* a speculative branch, subsequent tokens in that branch are discarded, and the request is marked as finished.

**Step 5: Memory Cleanup (KV Cache Management)**
*   **Eviction Mask**: A boolean mask `evict_mask` is created, marking all rejected tokens as `True`.
*   **Freeing Memory**:
    *   **Simple Case (Page Size=1 or TopK=1)**: Slots corresponding to the `evict_mask` are directly freed using `token_to_kv_pool_allocator.free`.
    *   **Complex Case (Page Size>1 & TopK>1)**: Rejected tokens can create fragmentation ("holes") within pages.
        *   The system calculates `tgt_cache_loc`, representing where accepted tokens *should* be to form contiguous blocks.
        *   It moves KV cache data from `src` (scattered) to `tgt` (compact) using `move_kv_cache`.
        *   Finally, the unused (now empty) slots are freed.
