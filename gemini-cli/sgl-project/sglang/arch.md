# SGLang MoE Runner Architecture

This document details the Mixture-of-Experts (MoE) runner modes supported in SGLang and their application scenarios.

## Supported MoE Runner Modes

SGLang supports **9 modes** (backends) for the MoE runner, defined in `MoeRunnerBackend`:

1.  **`auto`** (Default)
2.  **`deep_gemm`**
3.  **`triton`**
4.  **`triton_kernel`**
5.  **`flashinfer_trtllm`**
6.  **`flashinfer_cutlass`**
7.  **`flashinfer_mxfp4`**
8.  **`flashinfer_cutedsl`**
9.  **`cutlass`**

## Application Scenarios

These modes can be selected via the `--moe-runner-backend` server argument.

| Mode | Scenario & Application | Parallelism (MoE FFN) | Supported Quantization |
| :--- | :--- | :--- | :--- |
| **`auto`** | **General Usage.** This is the default setting. SGLang automatically selects the most appropriate backend based on your hardware (e.g., NVIDIA vs. AMD), installed libraries (`flashinfer`, `deep_gemm`), and model configuration (quantization type). | Auto-selected | Auto-selected |
| **`deep_gemm`** | **DeepSeek Models (Hopper+).** Specifically optimized for DeepSeek-V3 and DeepSeek-R1 models, typically delivering the highest performance on NVIDIA Hopper (H100/H200) and Blackwell GPUs. Requires the `deep_gemm` library. | Expert Parallel (EP) via DeepEP | FP8 (W8A8) |
| **`triton`** | **Standard / Portable.** A general-purpose backend using Triton kernels. It serves as a robust baseline and is often used when specific high-performance kernels (like FlashInfer) are unavailable or incompatible with the current setup. | Tensor Parallel (TP) | FP16/BF16, FP8 (W8A8), INT8 (W8A8), INT8 (W8A16), INT4 (W4A16) |
| **`triton_kernel`** | **Optimized Triton.** A variation of the Triton backend using specific optimized kernels (likely from `sgl-kernel` or internal implementations) for better performance than the standard Triton path in some cases. | Tensor Parallel (TP) | FP16/BF16, FP8, INT8, INT4 |
| **`flashinfer_trtllm`** | **High-Performance FP8/FP4.** Leverages FlashInfer with TensorRT-LLM kernels. This is critical for running quantized models like **DeepSeek-V3 in FP4**, providing significant speedups on supported hardware. | Tensor Parallel (TP) | FP8 (W8A8), FP4 (NVFP4) |
| **`flashinfer_cutlass`** | **FlashInfer + Cutlass.** Used when FlashInfer is available but Cutlass kernels are preferred for specific matrix multiplications, often for specific quantization formats or attention mechanisms coupled with MoE. | Expert Parallel (EP) + Tensor Parallel (TP) | FP4 (NVFP4) |
| **`flashinfer_mxfp4`** | **MXFP4 Quantization.** Specifically designed for models quantized using the MXFP4 (Microscaling formats) standard. | Expert Parallel (EP) + Tensor Parallel (TP) | MXFP4 |
| **`flashinfer_cutedsl`** | **Experimental / CuTe.** A backend using FlashInfer with CuTeDSL (CUDA Template Library), offering high-performance primitives for specific GPU architectures. | Expert Parallel (EP) + Tensor Parallel (TP) | FP4 (NVFP4) |
| **`cutlass`** | **FP8 / GEMM.** A generic backend using Cutlass kernels. It is often used for **FP8 MoE** implementations (`Fp8MoEMethod`) when FlashInfer is not used or for specific GEMM (General Matrix Multiply) operations. | Tensor Parallel (TP) | FP8 (W8A8), FP4 (NVFP4) |

## Configuration

You can explicitly set the backend when launching the server:

```bash
python -m sglang.launch_server --model-path <path> --moe-runner-backend deep_gemm
```

(Replace `deep_gemm` with your desired mode).

# Speculative Decoding Architecture

Speculative decoding in SGLang is implemented primarily through the **EAGLE** (Extrapolation Algorithm for Greater Language-model Efficiency) framework, alongside other variants like EAGLE3, NextN, and N-gram.

The implementation is split between the **Python runtime** (`python/sglang/srt`) which orchestrates the draft-verify loop, and **CUDA kernels** (`sgl-kernel`) which handle high-performance tree construction and sampling.

## Core Logic & Workflow

The process follows a "Draft-Verify" cycle managed by the `EAGLEWorker`.

### 1. Draft Phase (`EAGLEWorker.draft`)
*   The draft model (a smaller model) runs for `speculative_num_steps`.
*   At each step, it expands a tree of candidate tokens using `select_top_k_tokens`.
*   The tree structure is flattened into efficient tensors using the `build_tree_kernel_efficient` kernel.

### 2. Verify Phase (`EAGLEWorker.verify`)
The target model (the main LLM) validates the speculative tokens generated by the draft model.

Based on the code analysis, here is the detailed execution of the **Verify Process** in SGLang's speculative decoding (specifically the EAGLE implementation).

### Core Files & Functions

| Component | File Path | Key Function | Description |
| :--- | :--- | :--- | :--- |
| **Orchestrator** | `python/sglang/srt/speculative/eagle_worker.py` | `EAGLEWorker.verify` | Manages the overall flow: preparing the batch, calling the target model, and triggering the verification logic. |
| **Verification Logic** | `python/sglang/srt/speculative/eagle_info.py` | `EagleVerifyInput.verify` | Handles the input preparation, invokes the CUDA kernel, and performs **critical memory management** (freeing rejected KV cache slots). |
| **Kernel Interface** | `sgl-kernel/csrc/speculative/speculative_sampling.cu` | `tree_speculative_sampling_target_only` | The C++ entry point that exposes the CUDA kernel to Python. |
| **CUDA Kernel** | `sgl-kernel/csrc/speculative/speculative_sampling.cuh` | `TreeSpeculativeSamplingTargetOnly` | The GPU kernel that traverses the token tree and performs rejection sampling. |

---

### Detailed Data Process

#### 1. Preparation Phase (Python)
**File:** `eagle_worker.py` -> `EAGLEWorker.verify` calls `spec_info.prepare_for_verify`

The draft phase produces an `EagleVerifyInput` object containing candidate tokens and their tree structure.
1.  **Input Swap:** The batch's `input_ids` are replaced with `draft_token` (the list of all candidate tokens).
2.  **Optimistic Memory Allocation:** SGLang allocates KV cache slots for **all** candidate tokens immediately (`alloc_token_slots`), assuming they *might* all be accepted. This allows the attention backend to write K/V pairs for the candidates during the target model pass.
3.  **Mapping Update:** The `req_to_token_pool` is updated to map the requests to these new slots.

#### 2. Target Model Execution (Python/GPU)
**File:** `eagle_worker.py` -> `self.target_worker.forward_batch_generation`

1.  The **Target Model** runs a forward pass on the batch.
2.  Because `input_ids` now contains the entire tree of draft tokens, the target model computes logits for every candidate in parallel.
3.  **Output:** `logits_output` containing `next_token_logits` for every draft token.

#### 3. Verification & Sampling (Python -> CUDA)
**File:** `eagle_info.py` -> `EagleVerifyInput.verify`

1.  **Logit Processing:**
    *   Applies penalties (e.g., repetition penalty) and grammar masks (for structured output) to the `logits_output`.
    *   Computes `target_probs` via softmax.
2.  **Kernel Invocation:**
    *   Calls `tree_speculative_sampling_target_only`.
    *   **Inputs:**
        *   `candidates`: The draft token IDs.
        *   `target_probs`: Probabilities from the target model.
        *   `retrive_index`, `retrive_next_token`: Arrays defining the tree topology (parent-child relationships).
        *   `uniform_samples`: Random numbers for rejection sampling.

#### 4. The Verification Kernel (CUDA)
**File:** `speculative_sampling.cuh` -> `TreeSpeculativeSamplingTargetOnly`

1.  **Parallel Traversal:** Each thread block handles one request (one tree).
2.  **Tree Walking:** The kernel starts at the root of the draft tree and follows the `retrive_next_token` indices.
3.  **Rejection Sampling:** For each node:
    *   It compares the `target_prob` of the token against the random sample.
    *   **Accept:** If `target_prob >= threshold` (or based on the rejection sampling formula), the token is accepted. The kernel moves to the child node.
    *   **Reject:** If the condition fails, the branch stops.
4.  **Bonus Token:** After the accepted path ends, the kernel samples one additional token from the target model's distribution (the "bonus" token).
5.  **Output:** Writes the `accept_index` (indices of accepted tokens in the batch) and `predict` (the accepted token IDs).

#### 5. Post-Processing & Memory Cleanup (Python)
**File:** `eagle_info.py` -> `EagleVerifyInput.verify` (after kernel returns)

This is a critical performance step in SGLang's implementation.
1.  **State Update:** Accepted tokens are appended to the request's `output_ids`.
2.  **Eviction Mask:** A boolean `evict_mask` is created.
    *   `True` = Rejected token (garbage).
    *   `False` = Accepted token (keep).
3.  **Memory Freeing:**
    *   **Simple Case (Page Size=1 or TopK=1):** `token_to_kv_pool_allocator.free(evict_mask)` is called immediately to release slots used by rejected candidates.
    *   **Complex Case (Page Size > 1 & TopK > 1):** Rejected tokens might leave "holes" inside KV pages. SGLang uses `move_kv_cache` to compact the accepted tokens into contiguous pages before freeing the fragmented ones.
4.  **Next Draft Prep:** The hidden states of the *accepted* tokens are preserved to serve as the input for the next draft phase.


---

## Draft Batch Handling: Idle vs. Decode

### Function of `_draft_preprocess_idle`

The function `_draft_preprocess_idle` is a helper method used to prepare the batch state when the speculative worker is in an **IDLE** mode (i.e., not actively drafting tokens for verification).

Its specific actions are:
1.  **Creates Empty Input**: It initializes `batch.spec_info` with an empty `EagleDraftInput` object using `EagleDraftInput.create_idle_input`.
    *   This object contains zero-sized tensors for `verified_id`, `hidden_states`, `topk_p`, etc.
2.  **Prevents Computation**: By setting these empty structures, it ensures that the subsequent `draft_forward` loop operates on empty data (effectively a no-op), avoiding unnecessary memory allocation or kernel launches for the draft model.

```python
def _draft_preprocess_idle(self, batch: ScheduleBatch):
    batch.spec_info = EagleDraftInput.create_idle_input(
        device=self.device,
        hidden_size=self.model_config.hidden_size,
        dtype=self.model_config.dtype,
        topk=self.topk,
        capture_hidden_mode=CaptureHiddenMode.LAST,
    )
```

### Difference: Idle Draft Batch vs. Decode Draft Batch

The distinction lies in whether the worker intends to generate a speculative token tree or just pass through (e.g., for synchronization or empty steps).

| Feature | **Decode Draft Batch** (`_draft_preprocess_decode`) | **Idle Draft Batch** (`_draft_preprocess_idle`) |
| :--- | :--- | :--- |
| **Purpose** | Active speculative generation. The draft model will run for $N$ steps to propose candidate tokens. | No-op / Passthrough. The draft model does not generate tokens. |
| **Memory Allocation** | **Allocates KV Cache.** It calls `alloc_token_slots` (or paged variants) to reserve GPU memory for the new candidate tokens that will be generated. | **None.** No new KV cache slots are allocated. |
| **Input State** | Uses valid hidden states and token IDs from the previous verification step (or prefill) to seed the generation. | Uses empty (size 0) tensors for hidden states and IDs. |
| **Batch Mode** | `ForwardMode.DECODE` | `ForwardMode.IDLE` |
| **Outcome** | Produces a `SpecInput` with a tree of candidates to be verified by the target model. | Produces an empty `SpecInput`. The subsequent `verify` step will also simply return an empty result without running the target model. |

**When is IDLE used?**
An **IDLE** batch typically occurs in scenarios like:
1.  **Data Parallel Attention (DP Attention)**: If a specific worker in a DP group has no requests to process in the current step but needs to stay synchronized with the group.
2.  **Empty Batches**: If the scheduler sends a batch with no active requests for the speculative worker.
3.  **Initialization/Reset**: When transitioning states where no valid history exists to draft from immediately (though prefill usually handles the setup).

## Key Code Files

### Python Runtime (`python/sglang/srt/speculative/`)
*   **`eagle_worker.py`**: The central controller. Inherits from `TpModelWorker` and implements the `draft()` and `verify()` loops.
*   **`eagle_info.py`**: Defines core data structures (`EagleDraftInput`, `EagleVerifyInput`) and contains the `verify()` method calling the sampling kernels.
*   **`spec_info.py`**: Registry for speculative algorithms (`SpeculativeAlgorithm`).
*   **`spec_utils.py`**: Utility functions for tree expansion (`select_top_k_tokens`), memory assignment (`assign_req_to_token_pool`), and cache location management (`get_src_tgt_cache_loc`).

### Kernel Interface (`sgl-kernel/`)
*   **`python/sgl_kernel/speculative.py`**: Python bindings.
*   **`csrc/speculative/speculative_sampling.cu`**: CUDA implementation of rejection sampling (`TreeSpeculativeSamplingTargetOnly`) and tree construction.

## Core Data Structures

### `EagleVerifyInput`
Prepares the batch for the target model's verification pass.
*   `draft_token`: Candidate tokens proposed by the draft model.
*   `custom_mask`: Attention mask representing the tree structure.
*   `retrive_index`: Indices mapping tree nodes back to requests.
*   `retrive_next_token`: Tree topology (child node index).
*   `retrive_next_sibling`: Tree topology (sibling node index).

### `EagleDraftInput`
Maintains state during the autoregressive drafting steps.
*   `hidden_states`: Hidden states from the model.
*   `verified_id`: Tokens confirmed in the previous step.
*   `topk_p` / `topk_index`: Probabilities and indices of top-k candidates.

# Model Loading Architecture

This section describes how SGLang loads models, from the initial CLI command to the actual weight loading.

## Overview

The model loading process is orchestrated by the `SRT` (SGLang Runtime) engine. It involves several layers of abstraction, starting from the entry point, passing through the engine and scheduler, and finally reaching the model executor and loader.

## Calling Process

1.  **Entry Point (`launch_server.py`)**:
    The process starts at `python/sglang/launch_server.py`. It parses command-line arguments into `ServerArgs` and calls `run_server`.

2.  **Server Initialization (`http_server.py`)**:
    `launch_server` in `python/sglang/srt/entrypoints/http_server.py` is called. It initializes the `Engine` (or orchestrates the subprocesses via `_launch_subprocesses`).

3.  **Process Launch (`engine.py`)**:
    `_launch_subprocesses` in `python/sglang/srt/entrypoints/engine.py` sets up the distributed environment and launches the scheduler process(es) using `mp.Process(target=run_scheduler_process, ...)`.

4.  **Scheduler & Worker (`scheduler.py`)**:
    The `Scheduler` initializes the `TpModelWorker` (Tensor Parallel Model Worker).
    `TpModelWorker` initializes the `ModelRunner`.

5.  **Model Runner (`model_runner.py`)**:
    `ModelRunner` is responsible for model execution and loading.
    In `ModelRunner.__init__`, it calls `self.load_model()`.
    `load_model` creates a `LoadConfig` and calls `get_model` from `sglang.srt.model_loader`.

6.  **Model Loader (`model_loader/loader.py`)**:
    `get_model` (in `sglang.srt.model_loader/__init__.py`) selects the appropriate loader (e.g., `DefaultModelLoader`, `GGUFModelLoader`) using `get_model_loader`.
    The selected loader's `load_model` method is called to actually load the weights into the `torch.nn.Module`.

## Core Functions & Data Structures

### Data Structures

*   **`ServerArgs`** (`python/sglang/srt/server_args.py`): Contains all command-line arguments (e.g., `--model-path`, `--load-format`, `--tp-size`).
*   **`ModelConfig`** (`python/sglang/srt/configs/model_config.py`): Configuration specific to the model architecture (hidden size, vocab size, etc.).
*   **`LoadConfig`** (`python/sglang/srt/configs/load_config.py`): Configuration for how to load the model (load format, download directory).

### Key Functions

*   **`ModelRunner.load_model()`** (`python/sglang/srt/model_executor/model_runner.py`): The high-level method that coordinates the loading. It prepares the `LoadConfig` and calls `get_model`.
*   **`get_model()`** (`python/sglang/srt/model_loader/__init__.py`): A factory function that instantiates the correct loader and loads the model.
*   **`BaseModelLoader.load_model()`** (`python/sglang/srt/model_loader/loader.py`): The abstract method implemented by specific loaders.
    *   **`DefaultModelLoader.load_model`**: Loads weights from HF/disk (safetensors, bin, pt).
    *   **`_prepare_weights`**: Handles downloading from HF/ModelScope and identifying weight files.
