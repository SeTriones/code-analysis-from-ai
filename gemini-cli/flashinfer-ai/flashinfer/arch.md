# FlashInfer Architecture

## Supported Backends

FlashInfer supports multiple backends to maximize performance across different GPU architectures (e.g., Volta, Ampere, Hopper, Blackwell) and use cases (e.g., different data types, dense vs. sparse).

### Attention Backends
These backends implement various attention mechanisms (Decode, Prefill, Append, MLA).

*   **`fa2` (FlashAttention-2)**: Standard CUDA core implementation. Widely compatible with modern GPUs (Ampere+).
*   **`fa3` (FlashAttention-3)**: Optimized implementation for NVIDIA Hopper (SM90) architecture using Tensor Memory Accelerator (TMA) and Warp Group Matrix Multiply-Accumulate (WGMMA).
*   **`cudnn`**: Utilizes NVIDIA's cuDNN library for attention operations. often used for specific optimized paths or as a fallback.
*   **`cutlass`**: Implementations based on the CUTLASS library. often used for specialized cases like FP8 or specific hardware targets (e.g., Blackwell).
*   **`trtllm-gen`**: Kernels generated by TensorRT-LLM. Often used for Blackwell (SM100/SM103) or specific high-performance decode/prefill scenarios.
*   **`xqa`**: Optimized kernels for specific architectures like Hopper (SM90) and Blackwell (SM120).

### GEMM Backends
These backends handle General Matrix Multiplications, including low-precision (FP8, FP4) operations.

*   **`cublas`**: Standard NVIDIA cuBLAS library.
*   **`cutlass`**: CUTLASS-based GEMM implementations, crucial for FP8 and mixed-precision support.
*   **`cudnn`**: cuDNN-based GEMM.
*   **`trtllm`**: TensorRT-LLM based GEMM kernels.
*   **`deepgemm`**: specialized backend for DeepGEMM optimizations.

## Backend Selection Strategy

FlashInfer employs a flexible dispatching mechanism to select the most appropriate backend for a given operation and hardware.

### 1. Automatic Selection (`backend="auto"`)
This is the default mode. The system inspects the runtime environment to determine the best backend:
*   **Hardware Capability:** Checks the GPU Compute Capability (e.g., SM80, SM90, SM100).
*   **Feature Support:** Verifies if specific features (like TMA or specific instruction sets) are available.
*   **Heuristics:** Uses internal heuristic functions (e.g., `determine_attention_backend`, `determine_gemm_backend` in `flashinfer/utils.py`) to prioritize backends.
    *   *Example:* On an SM90 (Hopper) GPU, `auto` might prioritize `fa3` or `xqa` over `fa2`.
    *   *Example:* On SM100 (Blackwell), it might prioritize `trtllm-gen`.

### 2. Explicit Selection
Users can manually override the automatic selection by passing the `backend` argument to API functions.
*   **Example:** `flashinfer.single_decode_with_kv_cache(..., backend="fa2")` enforces the use of the FlashAttention-2 implementation.

### 3. Requirements and Validation
The `@backend_requirement` decorator (in `flashinfer/utils.py`) enforces constraints:
*   **Architecture Checks:** Ensures a selected backend is supported by the current hardware (e.g., preventing `fa3` usage on Ampere).
*   **Data Type/Shape Checks:** Verifies that the backend supports the requested precision (e.g., FP8) and tensor shapes.
